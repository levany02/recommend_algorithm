{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6796a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c281403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1447cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed9a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 14:16:12.745550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-21 14:16:12.745588: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-21 14:16:12.745613: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (l0137167181): /proc/driver/nvidia/version does not exist\n",
      "2021-10-21 14:16:12.745903: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n, d = 2048, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]\n",
    "\n",
    "# Juggle the dimensions for the plot\n",
    "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
    "pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n",
    "pos_encoding = tf.reshape(pos_encoding, (d, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e974f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d26b921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a752579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d012f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfad8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0527069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61637cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4524e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5d80da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10],\n",
    "                      [0, 10, 0],\n",
    "                      [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b218c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af644ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fb1dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8a36f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d557417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e69b51a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4d802a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f6f0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7ae2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca54e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb628ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90134948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28ed4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d99ba1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000,\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51a69f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42f1925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "093b98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e4fe2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cd2622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f02e5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00ac9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9da0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.randint(0, 50, size=(100, 5))\n",
    "target_data = np.random.randint(0, 50, size=(100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc629571",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=50,\n",
    "    target_vocab_size=50,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "224fe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb1a2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a28a6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp],\n",
    "                                     training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60875d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 4.4490 Accuracy 0.0127\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-1\n",
      "Epoch 1 Loss 4.4490 Accuracy 0.0127\n",
      "Time taken for 1 epoch: 0.28 secs\n",
      "\n",
      "Epoch 2 Loss 4.5005 Accuracy 0.0203\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-2\n",
      "Epoch 2 Loss 4.5005 Accuracy 0.0203\n",
      "Time taken for 1 epoch: 0.21 secs\n",
      "\n",
      "Epoch 3 Loss 4.4323 Accuracy 0.0381\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-3\n",
      "Epoch 3 Loss 4.4323 Accuracy 0.0381\n",
      "Time taken for 1 epoch: 0.21 secs\n",
      "\n",
      "Epoch 4 Loss 4.4852 Accuracy 0.0178\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-4\n",
      "Epoch 4 Loss 4.4852 Accuracy 0.0178\n",
      "Time taken for 1 epoch: 0.20 secs\n",
      "\n",
      "Epoch 5 Loss 4.5145 Accuracy 0.0102\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
      "Epoch 5 Loss 4.5145 Accuracy 0.0102\n",
      "Time taken for 1 epoch: 0.21 secs\n",
      "\n",
      "Epoch 6 Loss 4.4707 Accuracy 0.0203\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-6\n",
      "Epoch 6 Loss 4.4707 Accuracy 0.0203\n",
      "Time taken for 1 epoch: 0.22 secs\n",
      "\n",
      "Epoch 7 Loss 4.4442 Accuracy 0.0330\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-7\n",
      "Epoch 7 Loss 4.4442 Accuracy 0.0330\n",
      "Time taken for 1 epoch: 0.23 secs\n",
      "\n",
      "Epoch 8 Loss 4.4622 Accuracy 0.0203\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-8\n",
      "Epoch 8 Loss 4.4622 Accuracy 0.0203\n",
      "Time taken for 1 epoch: 0.43 secs\n",
      "\n",
      "Epoch 9 Loss 4.4475 Accuracy 0.0152\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-9\n",
      "Epoch 9 Loss 4.4475 Accuracy 0.0152\n",
      "Time taken for 1 epoch: 0.23 secs\n",
      "\n",
      "Epoch 10 Loss 4.4647 Accuracy 0.0203\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-10\n",
      "Epoch 10 Loss 4.4647 Accuracy 0.0203\n",
      "Time taken for 1 epoch: 0.24 secs\n",
      "\n",
      "Epoch 11 Loss 4.4369 Accuracy 0.0203\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/train/ckpt-11\n",
      "Epoch 11 Loss 4.4369 Accuracy 0.0203\n",
      "Time taken for 1 epoch: 0.25 secs\n",
      "\n",
      "Epoch 12 Loss 4.4290 Accuracy 0.0152\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/train/ckpt-12\n",
      "Epoch 12 Loss 4.4290 Accuracy 0.0152\n",
      "Time taken for 1 epoch: 0.21 secs\n",
      "\n",
      "Epoch 13 Loss 4.4193 Accuracy 0.0228\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/train/ckpt-13\n",
      "Epoch 13 Loss 4.4193 Accuracy 0.0228\n",
      "Time taken for 1 epoch: 0.25 secs\n",
      "\n",
      "Epoch 14 Loss 4.3722 Accuracy 0.0228\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/train/ckpt-14\n",
      "Epoch 14 Loss 4.3722 Accuracy 0.0228\n",
      "Time taken for 1 epoch: 0.21 secs\n",
      "\n",
      "Epoch 15 Loss 4.3012 Accuracy 0.0279\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-15\n",
      "Epoch 15 Loss 4.3012 Accuracy 0.0279\n",
      "Time taken for 1 epoch: 0.24 secs\n",
      "\n",
      "Epoch 16 Loss 4.4140 Accuracy 0.0178\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/train/ckpt-16\n",
      "Epoch 16 Loss 4.4140 Accuracy 0.0178\n",
      "Time taken for 1 epoch: 0.23 secs\n",
      "\n",
      "Epoch 17 Loss 4.3567 Accuracy 0.0305\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/train/ckpt-17\n",
      "Epoch 17 Loss 4.3567 Accuracy 0.0305\n",
      "Time taken for 1 epoch: 0.24 secs\n",
      "\n",
      "Epoch 18 Loss 4.3596 Accuracy 0.0228\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/train/ckpt-18\n",
      "Epoch 18 Loss 4.3596 Accuracy 0.0228\n",
      "Time taken for 1 epoch: 0.24 secs\n",
      "\n",
      "Epoch 19 Loss 4.3556 Accuracy 0.0254\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/train/ckpt-19\n",
      "Epoch 19 Loss 4.3556 Accuracy 0.0254\n",
      "Time taken for 1 epoch: 0.23 secs\n",
      "\n",
      "Epoch 20 Loss 4.3395 Accuracy 0.0279\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-20\n",
      "Epoch 20 Loss 4.3395 Accuracy 0.0279\n",
      "Time taken for 1 epoch: 0.24 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    train_step(input_data, target_data)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

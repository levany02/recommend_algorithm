{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "growing-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naval-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv(\n",
    "    '../data/ml-100k/ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')\n",
    "\n",
    "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(\n",
    "    '../data/ml-100k/ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
    "\n",
    "# The movies file contains a binary feature for each genre.\n",
    "genre_cols = [\n",
    "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "movies_cols = [\n",
    "    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
    "] + genre_cols\n",
    "movies = pd.read_csv(\n",
    "    '../data/ml-100k/ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
    "\n",
    "# Since the ids start at 1, we shift them to start at 0.\n",
    "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\n",
    "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\n",
    "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n",
    "\n",
    "# Since some movies can belong to more than one genre, we create different\n",
    "# 'genre' columns as follows:\n",
    "# - all_genres: all the active genres of the movie.\n",
    "# - genre: randomly sampled from the active genres.\n",
    "def mark_genres(movies, genres):\n",
    "    def get_random_genre(gs):\n",
    "        active = [genre for genre, g in zip(genres, gs) if g==1]\n",
    "        if len(active) == 0:\n",
    "            return 'Other'\n",
    "        return np.random.choice(active)\n",
    "    def get_all_genres(gs):\n",
    "        active = [genre for genre, g in zip(genres, gs) if g==1]\n",
    "        if len(active) == 0:\n",
    "            return 'Other'\n",
    "        return '-'.join(active)\n",
    "    movies['genre'] = [\n",
    "          get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
    "    movies['all_genres'] = [\n",
    "          get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
    "\n",
    "mark_genres(movies, genre_cols)\n",
    "\n",
    "# Create one merged DataFrame containing all the movielens data.\n",
    "movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "flexible-cooperation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[60, 188, 32, 159, 19, 201, 170, 264, 154, 116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[291, 250, 49, 313, 296, 289, 311, 280, 12, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[110, 557, 731, 226, 424, 739, 722, 37, 724, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[828, 303, 595, 221, 470, 404, 280, 251, 281, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>[767, 822, 69, 514, 523, 321, 624, 160, 447, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                           movie_id\n",
       "0       0  [60, 188, 32, 159, 19, 201, 170, 264, 154, 116...\n",
       "1       1  [291, 250, 49, 313, 296, 289, 311, 280, 12, 27...\n",
       "2      10  [110, 557, 731, 226, 424, 739, 722, 37, 724, 1...\n",
       "3     100  [828, 303, 595, 221, 470, 404, 280, 251, 281, ...\n",
       "4     101  [767, 822, 69, 514, 523, 321, 624, 160, 447, 4..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# years = movies['year'].unique()\n",
    "# years = np.delete(years, np.where(years=='nan'))\n",
    "# years = years.astype(np.float)\n",
    "\n",
    "# # Filling nan values\n",
    "# movies[\"year\"] = movies[\"year\"].apply(lambda s: \"1992\" if s==\"nan\" else s)\n",
    "# movies[\"year\"] = movies[\"year\"].astype(np.float)\n",
    "\n",
    "rated_movies = ratings[[\"movie_id\", \"user_id\"]].groupby([\"user_id\"], as_index=False).aggregate(lambda x: list(x))\n",
    "rated_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integrated-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_dict = {\n",
    "    movie: year for movie, year in zip(movies[\"movie_id\"], movies[\"year\"])\n",
    "}\n",
    "genres_dict = {\n",
    "    movie: genres.split('-')\n",
    "    for movie, genres in zip(movies[\"movie_id\"], movies[\"all_genres\"])\n",
    "}\n",
    "def make_batch(ratings, batch_size):\n",
    "    \"\"\"Creates a batch of examples.\n",
    "    Args:\n",
    "    ratings: A DataFrame of ratings such that examples[\"movie_id\"] is a list of\n",
    "      movies rated by a user.\n",
    "    batch_size: The batch size.\n",
    "    \"\"\"\n",
    "    def pad(x, fill):\n",
    "        return pd.DataFrame.from_dict(x).fillna(fill).values\n",
    "\n",
    "    movie = []\n",
    "    year = []\n",
    "    genre = []\n",
    "    label = []\n",
    "    for movie_ids in ratings[\"movie_id\"].values:\n",
    "        movie.append(movie_ids)\n",
    "        genre.append([x for movie_id in movie_ids for x in genres_dict[movie_id]])\n",
    "        year.append([years_dict[movie_id] for movie_id in movie_ids])\n",
    "        label.append([int(movie_id) for movie_id in movie_ids])\n",
    "    features = {\n",
    "      \"movie_id\": pad(movie, \"\"),\n",
    "      \"year\": pad(year, \"\"),\n",
    "      \"genre\": pad(genre, \"\"),\n",
    "      \"label\": pad(label, -1)\n",
    "    }\n",
    "    \n",
    "    batch = (\n",
    "      tf.data.Dataset.from_tensor_slices(features)\n",
    "      .shuffle(1000)\n",
    "      .repeat()\n",
    "      .batch(batch_size)\n",
    "#       .make_one_shot_iterator()\n",
    "#       .get_next()\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def select_random(x):\n",
    "    \"\"\"Selectes a random elements from each row of x.\"\"\"\n",
    "    def to_float(x):\n",
    "        return tf.cast(x, tf.float32)\n",
    "    def to_int(x):\n",
    "        return tf.cast(x, tf.int64)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    rn = tf.range(batch_size)\n",
    "    nnz = to_float(tf.math.count_nonzero(x >= 0, axis=1))\n",
    "    rnd = tf.random.uniform([batch_size])\n",
    "    ids = tf.stack([to_int(rn), to_int(nnz * rnd)], axis=1)\n",
    "    return to_int(tf.gather_nd(x, ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lonely-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "    \"\"\"Splits a DataFrame into training and test sets.\n",
    "    Args:\n",
    "    df: a dataframe.\n",
    "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "    Returns:\n",
    "    train: dataframe for training\n",
    "    test: dataframe for testing\n",
    "    \"\"\"\n",
    "    test = df.sample(frac=holdout_fraction, replace=False)\n",
    "    train = df[~df.index.isin(test.index)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wicked-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_col(key, embedding_dim):\n",
    "    categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=key, vocabulary_list=list(set(movies[key].values)), num_oov_buckets=0)\n",
    "    return tf.feature_column.embedding_column(\n",
    "        categorical_column=categorical_col, dimension=embedding_dim,\n",
    "        # default initializer: trancated normal with stddev=1/sqrt(dimension)\n",
    "        combiner='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "arctic-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(user_embeddings, movie_embeddings, labels):\n",
    "    \"\"\"Returns the cross-entropy loss of the softmax model.\n",
    "    Args:\n",
    "    user_embeddings: A tensor of shape [batch_size, embedding_dim].\n",
    "    movie_embeddings: A tensor of shape [num_movies, embedding_dim].\n",
    "    labels: A tensor of [batch_size], such that labels[i] is the target label\n",
    "      for example i.\n",
    "    Returns:\n",
    "    The mean cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Verify that the embddings have compatible dimensions\n",
    "    user_emb_dim = user_embeddings.shape[1]\n",
    "    movie_emb_dim = movie_embeddings.shape[1]\n",
    "    if user_emb_dim != movie_emb_dim:\n",
    "        raise ValueError(\n",
    "            \"The user embedding dimension %d should match the movie embedding \"\n",
    "            \"dimension % d\" % (user_emb_dim, movie_emb_dim))\n",
    "\n",
    "    logits = tf.matmul(user_embeddings, movie_embeddings, transpose_b=True)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=labels))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developmental-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds = make_batch(rated_movies, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "played-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': array([['60', '188', '32', ..., '', '', ''],\n",
       "        ['291', '250', '49', ..., '', '', ''],\n",
       "        ['110', '557', '731', ..., '', '', ''],\n",
       "        ...,\n",
       "        ['46', '162', '516', ..., '', '', ''],\n",
       "        ['3', '267', '78', ..., '', '', ''],\n",
       "        ['343', '353', '267', ..., '', '', '']], dtype=object),\n",
       " 'year': array([['1994', '1992', '1995', ..., '', '', ''],\n",
       "        ['1997', '1997', '1977', ..., '', '', ''],\n",
       "        ['1996', '1994', '1993', ..., '', '', ''],\n",
       "        ...,\n",
       "        ['1994', '1974', '1979', ..., '', '', ''],\n",
       "        ['1995', '1997', '1993', ..., '', '', ''],\n",
       "        ['1997', '1998', '1997', ..., '', '', '']], dtype=object),\n",
       " 'genre': array([['Drama', 'Animation', 'Comedy', ..., '', '', ''],\n",
       "        ['Drama', 'Comedy', 'Action', ..., '', '', ''],\n",
       "        ['Comedy', 'Romance', 'Drama', ..., '', '', ''],\n",
       "        ...,\n",
       "        ['Comedy', 'Drama', 'Comedy', ..., '', '', ''],\n",
       "        ['Action', 'Comedy', 'Drama', ..., '', '', ''],\n",
       "        ['Drama', 'Comedy', 'Romance', ..., '', '', '']], dtype=object),\n",
       " 'label': array([[ 60., 188.,  32., ...,  -1.,  -1.,  -1.],\n",
       "        [291., 250.,  49., ...,  -1.,  -1.,  -1.],\n",
       "        [110., 557., 731., ...,  -1.,  -1.,  -1.],\n",
       "        ...,\n",
       "        [ 46., 162., 516., ...,  -1.,  -1.,  -1.],\n",
       "        [  3., 267.,  78., ...,  -1.,  -1.,  -1.],\n",
       "        [343., 353., 267., ...,  -1.,  -1.,  -1.]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "economic-first",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds[\"movie_id\"][0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-gossip",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unusual-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cols=[\n",
    "          make_embedding_col(\"movie_id\", 35),\n",
    "          make_embedding_col(\"genre\", 3),\n",
    "          make_embedding_col(\"year\", 2),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pregnant-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0---loss: 7.4281826; test loss: 7.428577\n",
      "iter10---loss: 7.427947; test loss: 7.4271917\n",
      "iter20---loss: 7.427975; test loss: 7.4283676\n",
      "iter30---loss: 7.4277005; test loss: 7.4290824\n",
      "iter40---loss: 7.427827; test loss: 7.4280076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-414a27237dae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_rated_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rated_movies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrated_movies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_rated_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtest_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rated_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-32376659aa42>\u001b[0m in \u001b[0;36mmake_batch\u001b[1;34m(ratings, batch_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     batch = (\n\u001b[1;32m---> 35\u001b[1;33m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \"\"\"\n\u001b[1;32m--> 691\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   3153\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3154\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3155\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3157\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    127\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m           normalized_components.append(\n\u001b[1;32m--> 129\u001b[1;33m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[0;32m    130\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_layers = tf.keras.layers.DenseFeatures(embedding_cols, trainable=True)\n",
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=1./np.sqrt(35))\n",
    "values = initializer(shape=(40,35))/10.\n",
    "w = tf.Variable(values, trainable=None)\n",
    "optimizer = tf.optimizers.Adagrad(learning_rate=8.)\n",
    "trainable_weights = feature_layers.trainable_weights\n",
    "# trainable_weights = feature_layers.trainable_weights\n",
    "for i in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        train_rated_movies, test_rated_movies = split_dataframe(rated_movies)\n",
    "        train_batch = make_batch(train_rated_movies, 200)\n",
    "        test_batch = make_batch(test_rated_movies, 100)\n",
    "        #train\n",
    "        inputs = feature_layers(train_batch)\n",
    "        outputs = tf.matmul(inputs, w)\n",
    "        labels = select_random(train_batch[\"label\"])\n",
    "#         print(feature_layers.weights, feature_layers.with_name_scope)\n",
    "        loss = softmax_loss(outputs, feature_layers.weights[1].numpy(), labels)\n",
    "        \n",
    "        #test\n",
    "        inputs_test = feature_layers(test_batch)\n",
    "        outputs_test = tf.matmul(inputs_test, w)\n",
    "        labels_test = select_random(test_batch[\"label\"])\n",
    "        loss_test = softmax_loss(outputs_test, feature_layers.weights[1].numpy(), labels_test)\n",
    "        \n",
    "        if i%10==0:\n",
    "            print(\"iter\"+str(i)+\"---loss: \" + str(loss.numpy()) + \"; test loss: \" + str(loss_test.numpy()))\n",
    "    grads = tape.gradient(loss, trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "greater-stuff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dict_keyiterator at 0x1b260456770>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conscious-certificate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40, 35), dtype=float32, numpy=\n",
       "array([[ 0.00908667, -0.00389983,  0.00691063, ...,  0.01126931,\n",
       "        -0.00636813, -0.00505206],\n",
       "       [ 0.02162261, -0.00619991,  0.01322017, ...,  0.00556174,\n",
       "        -0.03104937,  0.00985521],\n",
       "       [ 0.00211462, -0.00412473,  0.00116795, ..., -0.03139753,\n",
       "        -0.02429502,  0.03321886],\n",
       "       ...,\n",
       "       [-0.00427012,  0.01636573, -0.00585202, ...,  0.00477287,\n",
       "        -0.00790855, -0.02028035],\n",
       "       [ 0.0181758 , -0.01303392,  0.02147099, ..., -0.00164768,\n",
       "        -0.01768864, -0.0032793 ],\n",
       "       [ 0.00325453,  0.02362872,  0.01909943, ..., -0.00799608,\n",
       "         0.02900949, -0.01466182]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=1./np.sqrt(35))\n",
    "values = initializer(shape=(40,35))/10.\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reflected-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(object):\n",
    "    \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        \"\"\"Initializes a CFModel.\n",
    "        Args:\n",
    "          embedding_vars: A dictionary of tf.Variables.\n",
    "          loss: A float Tensor. The loss to optimize.\n",
    "          metrics: optional list of dictionaries of Tensors. The metrics in each\n",
    "            dictionary will be plotted in a separate figure during training.\n",
    "        \"\"\"\n",
    "        self._embedding_vars = embedding_vars\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        self._embeddings = {k: None for k in embedding_vars}\n",
    "        self._session = None\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        \"\"\"The embeddings dictionary.\"\"\"\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
    "            optimizer=tf.compat.v1.train.GradientDescentOptimizer):\n",
    "        \"\"\"Trains the model.\n",
    "        Args:\n",
    "          iterations: number of iterations to run.\n",
    "          learning_rate: optimizer learning rate.\n",
    "          plot_results: whether to plot the results at the end of training.\n",
    "          optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
    "        Returns:\n",
    "          The metrics dictionary evaluated at the last iteration.\n",
    "        \"\"\"\n",
    "        with self._loss.graph.as_default():\n",
    "            opt = optimizer(learning_rate)\n",
    "            train_op = opt.minimize(self._loss)\n",
    "            local_init_op = tf.group(\n",
    "              tf.compat.v1.variables_initializer(opt.variables()),\n",
    "              tf.compat.v1.local_variables_initializer())\n",
    "            if self._session is None:\n",
    "                self._session = tf.compat.v1.Session()\n",
    "                with self._session.as_default():\n",
    "                    self._session.run(tf.compat.v1.global_variables_initializer())\n",
    "                    self._session.run(tf.compat.v1.tables_initializer())\n",
    "                    tf.compat.v1.train.start_queue_runners()\n",
    "\n",
    "        with self._session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations = []\n",
    "            metrics = self._metrics or ({},)\n",
    "            metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
    "        # Train and append results.\n",
    "        for i in range(num_iterations + 1):\n",
    "            _, results = self._session.run((train_op, metrics))\n",
    "            if (i % 10 == 0) or i == num_iterations:\n",
    "                print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                    [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                end='')\n",
    "                iterations.append(i)\n",
    "                for metric_val, result in zip(metrics_vals, results):\n",
    "                    for k, v in result.items():\n",
    "                        metric_val[k].append(v)\n",
    "\n",
    "        for k, v in self._embedding_vars.items():\n",
    "            self._embeddings[k] = v.eval()\n",
    "\n",
    "#       if plot_results:\n",
    "#         # Plot the metrics.\n",
    "#         num_subplots = len(metrics)+1\n",
    "#         fig = plt.figure()\n",
    "#         fig.set_size_inches(num_subplots*10, 8)\n",
    "#         for i, metric_vals in enumerate(metrics_vals):\n",
    "#           ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "#           for k, v in metric_vals.items():\n",
    "#             ax.plot(iterations, v, label=k)\n",
    "#           ax.set_xlim([1, num_iterations])\n",
    "#           ax.legend()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "respective-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_softmax_model_2(rated_movies, embedding_cols, hidden_dims):\n",
    "    def create_network(features):\n",
    "        \"\"\"Maps input features dictionary to user embeddings.\n",
    "        Args:\n",
    "          features: A dictionary of input string tensors.\n",
    "        Returns:\n",
    "          outputs: A tensor of shape [batch_size, embedding_dim].\n",
    "        \"\"\"\n",
    "        # Create a bag-of-words embedding for each sparse feature.\n",
    "        inputs = tf.compat.v1.feature_column.input_layer(features, embedding_cols)\n",
    "        print(inputs)\n",
    "        # Hidden layers.\n",
    "        input_dim = inputs.shape[1]\n",
    "        for i, output_dim in enumerate(hidden_dims):\n",
    "#             w = tf.compat.v1.get_variable(\n",
    "#               \"hidden%d_w_\" % i, shape=[input_dim, output_dim],\n",
    "#               initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "#                   stddev=1./np.sqrt(output_dim))) / 10.\n",
    "            initializer = tf.keras.initializers.TruncatedNormal(1./np.sqrt(output_dim))\n",
    "            values = initializer(shape=[input_dim, output_dim])/10.\n",
    "            w = tf.Variable(values, trainable=None)\n",
    "            outputs = tf.matmul(inputs, w)\n",
    "            input_dim = output_dim\n",
    "            inputs = outputs\n",
    "        return outputs\n",
    "\n",
    "    train_rated_movies, test_rated_movies = split_dataframe(rated_movies)\n",
    "    train_batch = make_batch(train_rated_movies, 200)\n",
    "    test_batch = make_batch(test_rated_movies, 100)\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=False):\n",
    "        # Train\n",
    "        train_user_embeddings = create_network(train_batch)\n",
    "        train_labels = select_random(train_batch[\"label\"])\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=True):\n",
    "        # Test\n",
    "        test_user_embeddings = create_network(test_batch)\n",
    "        test_labels = select_random(test_batch[\"label\"])\n",
    "        #     print(\"*\"*80, inputs)\n",
    "        movie_embeddings = tf.compat.v1.get_variable(\n",
    "            \"input_layer/movie_id_embedding/embedding_weights\")\n",
    "\n",
    "    test_loss = softmax_loss(\n",
    "      test_user_embeddings, movie_embeddings, test_labels)\n",
    "    train_loss = softmax_loss(\n",
    "      train_user_embeddings, movie_embeddings, train_labels)\n",
    "    _, test_precision_at_10 = tf.compat.v1.metrics.precision_at_k(\n",
    "      labels=test_labels,\n",
    "      predictions=tf.matmul(test_user_embeddings, movie_embeddings, transpose_b=True),\n",
    "      k=10)\n",
    "\n",
    "    metrics = (\n",
    "      {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "      {\"test_precision_at_10\": test_precision_at_10}\n",
    "    )\n",
    "    embeddings = {\"movie_id\": movie_embeddings}\n",
    "    return CFModel(embeddings, train_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "announced-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_softmax_model(rated_movies, embedding_cols, hidden_dims):\n",
    "    def create_network(features):\n",
    "        \"\"\"Maps input features dictionary to user embeddings.\n",
    "        Args:\n",
    "          features: A dictionary of input string tensors.\n",
    "        Returns:\n",
    "          outputs: A tensor of shape [batch_size, embedding_dim].\n",
    "        \"\"\"\n",
    "        # Create a bag-of-words embedding for each sparse feature.\n",
    "        inputs = tf.compat.v1.feature_column.input_layer(features, embedding_cols)\n",
    "        print(inputs)\n",
    "        # Hidden layers.\n",
    "        input_dim = inputs.shape[1]\n",
    "        for i, output_dim in enumerate(hidden_dims):\n",
    "            w = tf.compat.v1.get_variable(\n",
    "              \"hidden%d_w_\" % i, shape=[input_dim, output_dim],\n",
    "              initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                  stddev=1./np.sqrt(output_dim))) / 10.\n",
    "            outputs = tf.matmul(inputs, w)\n",
    "            input_dim = output_dim\n",
    "            inputs = outputs\n",
    "        return outputs\n",
    "\n",
    "    train_rated_movies, test_rated_movies = split_dataframe(rated_movies)\n",
    "    train_batch = make_batch(train_rated_movies, 200)\n",
    "    test_batch = make_batch(test_rated_movies, 100)\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=False):\n",
    "        # Train\n",
    "        train_user_embeddings = create_network(train_batch)\n",
    "        train_labels = select_random(train_batch[\"label\"])\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=True):\n",
    "        # Test\n",
    "        test_user_embeddings = create_network(test_batch)\n",
    "        test_labels = select_random(test_batch[\"label\"])\n",
    "        #     print(\"*\"*80, inputs)\n",
    "        movie_embeddings = tf.compat.v1.get_variable(\n",
    "            \"input_layer/movie_id_embedding/embedding_weights\")\n",
    "\n",
    "    test_loss = softmax_loss(test_user_embeddings, movie_embeddings, test_labels)\n",
    "    train_loss = softmax_loss(train_user_embeddings, movie_embeddings, train_labels)\n",
    "    _, test_precision_at_10 = tf.compat.v1.metrics.precision_at_k(\n",
    "          labels=test_labels,\n",
    "          predictions=tf.matmul(test_user_embeddings, movie_embeddings, transpose_b=True),\n",
    "          k=10)\n",
    "\n",
    "    metrics = (\n",
    "      {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "      {\"test_precision_at_10\": test_precision_at_10}\n",
    "    )\n",
    "    embeddings = {\"movie_id\": movie_embeddings}\n",
    "    return CFModel(embeddings, train_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daily-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/input_layer/concat:0\", shape=(849, 40), dtype=float32)\n",
      "Tensor(\"model_1/input_layer/concat:0\", shape=(94, 40), dtype=float32)\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
      " iteration 1: train_loss=7.428047, test_loss=7.429975, test_precision_at_10=0.000000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to eval in EAGER mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-15f458d2123d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m       hidden_dims=[35])\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m softmax_model.train(\n\u001b[0m\u001b[0;32m     12\u001b[0m     learning_rate=8., num_iterations=1, optimizer=tf.compat.v1.train.AdagradOptimizer)\n",
      "\u001b[1;32m<ipython-input-10-5292b2a51c0d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_iterations, learning_rate, plot_results, optimizer)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedding_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#       if plot_results:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;34m\"\"\"Evaluates and returns the value of this variable.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trying to eval in EAGER mode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to eval in EAGER mode"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "  softmax_model = build_softmax_model(\n",
    "      rated_movies,\n",
    "      embedding_cols=[\n",
    "          make_embedding_col(\"movie_id\", 35),\n",
    "          make_embedding_col(\"genre\", 3),\n",
    "          make_embedding_col(\"year\", 2),\n",
    "      ],\n",
    "      hidden_dims=[35])\n",
    "\n",
    "softmax_model.train(\n",
    "    learning_rate=8., num_iterations=1, optimizer=tf.compat.v1.train.AdagradOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "gorgeous-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.train.AdagradOptimizer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
